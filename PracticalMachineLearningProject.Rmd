---
title: "Practical Machine Learning Project"
author: "Chen Yiguo"
date: "4/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, I will use the available features to predict the 'classe' in the data files. Particularly, I will use decision tree and random forests for the prediction. Apart from the different models, I will also investigate if there is any performance difference between omitting features with NA and keeping the features by imputing the NA observations.

## Load library and data

Caret library is used for the study.
```{r load}
library(caret)
df_test = read.csv('./pml-testing.csv')
df_train = read.csv('./pml-training.csv')

col_names = colnames(df_train)
col_names
str(df_train)
```

## Train test split and preprocess
As the column names suggest, the first 7 columns are index, use id, time, etc., which are not directly related to the 'classe' and shall not be used to predict the 'classe'. These 7 columns are removed, and 75% train-test split is used.

```{r split}
df_train = df_train[,-c(1:7)]
dim(df_train)
df_test = df_test[,-c(1:7)]
dim(df_test)
set.seed(123)
inTrain = createDataPartition(y = df_train$classe, p = 0.75, list = F)
training = df_train[inTrain,]
testing = df_train[-inTrain,]
```

Features with Near-zero variation are also exluded for the prediction. Because some of the features have NA, two groups of training data are generated based on how the NA values are dealt with. Group one (training_nzv_impute) keeps all the features after exluding near-zero variation features, and use 'knnImpute' method to impute the missing values. Group two (training_nzv_rmNA) ignores the features if NA exists in any observations

```{r preprocess, cache=TRUE}
nsv = nearZeroVar(training, saveMetrics = T)
training_nzv = training[,nsv$nzv == F]

preObj = preProcess(training_nzv, method = 'knnImpute')
training_nzv_impute = predict(preObj, training_nzv)

training_nzv_rmNA = training_nzv[,apply(is.na(training_nzv), MARGIN = 2, FUN = any) == F]
```

## Decision Tree model

Decision tree is employed for the prediction.
```{r rpart, cache=TRUE}
set.seed(123)
# for removing NA
modFit_rmNA = train(classe~.,  data = training_nzv_rmNA, method = 'rpart')
ybar_training_rmNA = predict(modFit_rmNA, training_nzv_rmNA)
# for imputing
modFit_impute = train(classe~.,  data = training_nzv_impute, method = 'rpart')
ybar_training_impute = predict(modFit_impute, training_nzv_impute)
```

```{r rpart_result, cache=TRUE}
# training results of removing NAs
cm_training_rmNA = confusionMatrix(ybar_training_rmNA, training_nzv_rmNA$classe)
cm_training_rmNA
# training results of imputing
cm_training_impute = confusionMatrix(ybar_training_impute, training_nzv_impute$classe)
cm_training_impute
```

Based on the summary of the data used for training, both groups show poor accuracy (~0.5), and the difference in accuracy is small. It seems decision tree is not a good method in this project.

## Random Forest Model

Random forest is employed in this section. To limit the time for training, cross-validation with 3 folds is set in trControl.
```{r rf, cache=T}
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
# removing NA
modRF_rmNA <- train(classe ~ ., data=training_nzv_rmNA, method="rf", trControl=controlRF)
ybar_training_rmNA_rf = predict(modRF_rmNA, newdata = training_nzv_rmNA)
# imputing
modRF_impute <- train(classe ~ ., data=training_nzv_impute, method="rf", trControl=controlRF)
ybar_training_impute_rf = predict(modRF_impute, newdata = training_nzv_impute)
```
```{r rf_results, cache=T}
# training results of removing NAs
cm_training_rmNA_rf = confusionMatrix(ybar_training_rmNA_rf, training_nzv_rmNA$classe)
cm_training_rmNA_rf
# training results of imputing
cm_training_impute_rf = confusionMatrix(ybar_training_impute_rf, training_nzv_impute$classe)
cm_training_impute_rf
```
```{r rf_testing, cache = T}
# training results of removing NAs
ybar_testing_rmNA_rf = predict(modRF_rmNA, newdata = testing)
cm_testing_rmNA_rf = confusionMatrix(ybar_testing_rmNA_rf, testing$classe)
cm_testing_rmNA_rf
# training results of imputing
testing_impute = predict(preObj, testing)
ybar_testing_impute_rf = predict(modRF_impute, newdata = testing_impute)
cm_testing_impute_rf = confusionMatrix(ybar_testing_impute_rf, testing$classe)
cm_testing_impute_rf
```

From the summary of the two random forest models and both training and testing data, high accuracy values (~0.99) have been achieved. And it seems the difference between ignoring NA or imputing NA is small.

## Predict pml-training.csv
Since the difference between ignoring NA or imputing NA is small, only one method is used in the prediction of pml-training.csv. 
```{r prediction, cache = T}
# removing NA
ybar_df_test_rmNA_rf = predict(modRF_rmNA, newdata = df_test)
ybar_df_test_rmNA_rf
```